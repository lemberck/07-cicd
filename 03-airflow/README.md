# Essential MLOps Platform Infrastructure with Containerized Airflow

## Overview
This MLOps project implements a foundational workflow for machine learning operations using Airflow, focusing on model training and inference within a local containerized environment. The objective is to ensure robustness and traceability throughout the machine learning lifecycle.

## Features
- **Model Registry**: Implements a model registry to store and manage model binaries and metadata, enabling version control and historical tracking.
- **Metadata Logging**: Comprehensive logging of model metadata, inference outputs, and error messages to facilitate transparency and ease of troubleshooting.
- **Error Handling**: Designed to halt the model training and inference processes in case of errors, preventing propagation of potentially flawed models.
    - Errors are registered in the logs, and no model is generated. 
- **Containerized Airflow Setup**: Utilizes `docker` and `docker-compose` to create a consistent and isolated environment for running `Airflow`, simplifying setup and deployment.

## Pipelines

### Training Pipeline (`prd_train_model_dag`)
> This DAG orchestrates the training of a Linear Regression model using scikit-learn, encompassing data preparation and model training stages with robust error handling. **2 Tasks** : *preprocessing* , *train_model*

The production training pipeline automates the end-to-end process of training a machine learning model. It starts with a data preprocessing step that manipulates and prepares the data for training. Following this, the Linear Regression model from the scikit-learn library is trained. Models are stored in a dedicated model registry (`models/`) with a unique filename that combines a timestamp and a hash of the model, ensuring traceability and uniqueness for each training run.

The `log.txt` file within the model registry directory logs essential metadata for each trained model, such as the hash, the model's filename, and the timestamp of the training. This metadata facilitates model versioning and environment tracking, crucial for MLOps practices.

Error handling is a key aspect of this pipeline. If any step in the training process encounters an issue, the error details, including a timestamp, are logged in `log.txt`. 

In case of an error, the pipeline is designed **not to generate or store a faulty model**, thereby ensuring the integrity of the model registry. The use of `raise` within the error handling ensures that Airflow captures the task failure, maintaining the pipeline's reliability.

> Note: A simple model was used as well as the hyperparameters tuning phase was skipped, due to not being the focus of the project.

### Inference Pipeline (`prd_inference_pipe_dag`)
> This DAG manages the inference process, applying the latest trained model to new data and logging the results, along with detailed inference metadata. **3 Tasks** : *generate_inf_data* , *preprocess_inf_data* , *model_inference*

The production inference pipeline is designed to operationalize machine learning models for generating predictions on new data inputs. It starts with the generation of inference data and applying the same data transformation as the training phase, after which it identifies and loads the most recent production-ready model from the model registry (`models/`) generated by the first DAG. This is done by scanning the `log.txt` for the latest entry with a matching environment tag.

Inference results, along with the timestamp, the environment of execution, the path of the input inference data, and the name of the model used, are systematically logged in `log_inference.txt`. This log file serves as an inference registry, offering a comprehensive audit trail of model applications and outcomes, essential for monitoring and compliance in MLOps workflows.

The DAG has been built with fault tolerance in mind. In case of any exceptions during the inference task, the error is captured with its timestamp and detailed error message in the `log_inference.txt` file. This ensures transparency and traceability of operational issues while preventing the persistence of erroneous inference results. The pipeline uses the `raise` statement post-error logging to notify Airflow of the task's unsuccessful execution, maintaining the robustness of the MLOps cycle.

> Note: The generated data is the same per run, as that is not the focus of the project. So, it is expected to find the same inference data at the logs inference registry.

## Project Structure
- **`dags/`**: DAG scripts for training and inference. 
    - Contains scripts for both dev and prd environment.
- **`models/`**: Serialized models, training metadata (`log.txt`) and inference metadata (`log_inference.txt`).
- **`scripts/`**: Supporting scripts for Airflow setup.
- **`Dockerfile` & `docker-compose.yaml`**: Docker configurations for the Airflow setup.
- **`requirements.txt`**: Project dependencies.

## Usage
Ensure Docker and docker-compose (v2.20.2) are installed and execute the following command at the project root:

- **Just** runner
```bash
just up
```

OR

- Docker-compose 
```bash
docker-compose build && docker-compose up -d airflow-init && docker-compose up -d
```