from datetime import datetime
from airflow.decorators import dag, task

# Define default arguments for the DAG
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
}

# Use the dag decorator to instantiate a DAG object
@dag(
    "dev_train_model_dag",  # Unique identifier for the DAG
    default_args=default_args,  # Link to the default arguments defined above
    description="A DAG for training a scikit-learn model",  # Provide a description for the DAG, goes to metadata.
    catchup=False,  # If True, Airflow will backfill past runs when the DAG is turned on, False prevents backfilling
    schedule=None,  # Set the schedule interval (None for one-time manual run or a cron expression)
    start_date=datetime(2023, 11, 13),  # Set the start date for the DAG to begin running
    tags=["training", "scikit-learn", "dev"],  # Tag the DAG for easier filtering in the UI
)
def train_model_dag():
    """A DAG for training a ML model"""

    ################ Task to perform data preprocessing
    @task.virtualenv(
        task_id="preprocessing",  # Unique identifier for the task
        requirements=["pandas",],  # List of requirements to install in the virtualenv
        system_site_packages=False,  # If False, a fresh virtual environment is created without system packages
    )

    def preprocessing():
        """Preprocessing the data required for model training.

        It creates a mock pandas DataFrame with features and a target variable. 
        Then, it performs a simple data manipulation on the 'feature1' column.
        Finally, it saves the DataFrame to a CSV file located at '/tmp/data.csv' and 
        returns the file path. 
        This path will be used as input for the model training task.

        Note: The use of the '/tmp' directory for storing the CSV file is suitable for this example 
        where the DAG is assumed to be running on a single machine. In a production scenario, 
        especially in distributed environments like those used by Airflow, it's important to use 
        a shared or central storage system (like a cloud storage bucket) for such data. This ensures 
        that the data is accessible to other tasks or workers that might be running on different machines.

        Returns:
            str: Path to the preprocessed data csv file
        
        """
        import pandas as pd  # Import pandas within the virtualenv
        import logging
    
        try:
            # Create a dataframe with mock data
            df = pd.DataFrame(
                {"feature1": range(20), "feature2": range(20, 40), "target": range(40, 60)}
            )
            df["feature1"] = df["feature1"] * 2  # Manipulate data as part of preprocessing

            # Save the dataframe to a csv file
            # using /tmp only works because it's the same machine.
            # If using different machines, there should be a common cloud storage
            df.to_csv("/tmp/data.csv", index=False)
            return "/tmp/data.csv"  # Return the path to the csv file
        except Exception as e:
            logging.error(f"Error in preprocessing: {e}")
            raise

    ################ Task to train the machine learning model
    @task.virtualenv(
        task_id="train_model",  # Unique identifier for the task
        requirements=["pandas", "scikit-learn", "joblib"],  # List of requirements to install in the virtualenv
        system_site_packages=False,  # If False, a fresh virtual environment is created without system packages
    )
    def train(csv_path: str):
        """
        Trains a machine learning model using the preprocessed data.

        Args:
            csv_path (str): Path to the preprocessed data csv file, which is the output from the 'preprocessing' task.

        This task reads the CSV file into a pandas DataFrame and then separates the features 
        and the target variable. It then creates and trains a LinearRegression model from 
        scikit-learn with the data. After training, the model is hashed, and its filename 
        is generated by combining the hash and the current timestamp to ensure uniqueness. 
        This file is saved in the '/opt/airflow/models/' directory (model registry). 
        
        The hash, filename, and training timestamp are also recorded in a log file in the same
        directory for future reference (model metadata).
        This approach ensures traceability and versioning of the trained models. 
        
        The task returns True to indicate successful completion of the training process.

        Returns:
            bool: True if the training was successful.
    """
        try:
            import joblib  # Library for saving and loading models
            import pandas as pd  # Import pandas within the virtualenv
            from sklearn.linear_model import LinearRegression  # Import the model class
            import logging
            import json
            from datetime import datetime

            # Define the environment
            env='dev' 

            # Load the data from the csv
            df = pd.read_csv(csv_path)

            # Separate features and target variable
            X = df[["feature1", "feature2"]]
            y = df["target"]

            # Instantiate the model and fit it to the data
            model = LinearRegression()
            model.fit(X, y)

            # Generate a hash for the model to uniquely identify it in the model registry
            model_hash = joblib.hash(model)

            # Create a timestamp for when the model was trained
            training_timestamp = datetime.now().strftime("%Y-%m-%d:%H-%M-%S")

            # Model Registry path
            mr_path=f'/opt/airflow/models'

            # create model's pickle filename
            model_filename = f"model_{training_timestamp}_{model_hash}.pkl"
            
            # Record the model's hash, filename, training time and environment in a json log file
            with open("/opt/airflow/models/log.txt", "a") as f:
                f.write(
                    json.dumps(
                        {
                        "model_hash": model_hash,
                        "model_filename": model_filename,
                        "training_timestamp": training_timestamp,
                        "environment": env
                        }
                    ) + "\n"  # Add a newline character so the next log is in a new line
                )
            
            # Save the trained model as a pickle file with a filename that includes the hash and timestamp
            joblib.dump(model, f'{mr_path}/{model_filename}')

            return True  # Return True to indicate success
        
        except Exception as e:
            logging.error(f"Error in training: {e}")
            error_timestamp = datetime.now().strftime("%Y-%m-%d:%H-%M-%S")
            with open("/opt/airflow/models/log.txt", "a") as f:
                f.write(
                    json.dumps(
                    {   "error_timestamp": error_timestamp,
                        "detail": str(e)
                        }
                    ) + "\n")
            raise # ensures that the error is not suppressed in airflow, so the task fails.

    ################ Define the task flow
    preprocessing_op = preprocessing()  # Call the preprocessing task
    train_op = train(preprocessing_op)  # Call the train task with the output of preprocessing


# Instantiate the DAG by calling the DAG function
train_model_dag()

"""
- Each task is wrapped in a try-except block.
- If an error occurs, it's logged using logging.error.
- In the train task, if an error occurs that impacts the model's metadata, 
an "ERROR" record is added to the log file.
- The raise statement in the except blocks ensures that the error is not 
suppressed and that Airflow is aware of the task failure.
"""

"""
joblib.hash function is intended to hash the contents of the model object, 
and if the training data and model parameters are the same across runs, it's 
possible that the hash would be identical. This is ok so you won't generate
equal models each run, but for the exercise purpose I've added a timestamp
so i could have "different" models in the model registry without changing
model's parameters.
"""